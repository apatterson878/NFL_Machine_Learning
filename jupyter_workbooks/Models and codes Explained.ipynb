{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes and Models exlained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project : Prediction of NFL game outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models used : Logistic regression, XGBoost, Optimized Gradient Booster, Support Vector Classification (SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor(X)= 28 Features reduced to 13 (after studying correlation matrix, and dropping highly correlated ones)\n",
    "\n",
    "Predicted(y)= Whether the team chosen will win or lose in the season selected\n",
    "\n",
    "We use average of previous two years for each feature to make our predictions for the current season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#side note:Matrices that contain mostly zero values are called sparse,\n",
    "#distinct from matrices where most of the values are non-zero, called dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is used to predict log-odds (ln(p/1-p)) of a binary event like win/lose, pass/fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the function that converts log-odds to probability is the logistic function\n",
    "##### Logistic function f(x)= 1/1+e^-xB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The unit of measurement for the log-odds scale is called a logit, from logistic unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost stands for eXtreme Gradient Boosting.\n",
    "\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "\n",
    "    Execution Speed.\n",
    "    Model Performance.\n",
    "    \n",
    "The XGBoost library implements the gradient boosting decision tree algorithm.\n",
    "\n",
    "Boosting is an ensemble technique where new models are added to correct the errors made by existing models. \n",
    "Models are added sequentially until no further improvements can be made. \n",
    "\n",
    "It builds the model in a stage-wise fashion like other boosting methods do,\n",
    "and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n",
    "(Training Loss and Validation Loss) \n",
    "\n",
    " The logistic function is typically used for binary classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset consisting of features set and labels set,\n",
    "an SVM classifier builds a model to predict classes for new examples.\n",
    "It assigns new example/data points to one of the classes.\n",
    "If there are only 2 classes then it can be called as a Binary SVM Classifier.\n",
    "SVMS are a byproduct of Neural Network. They are widely applied to pattern classification and regression problems.\n",
    "\n",
    "Default Kernel:\n",
    "    \n",
    "Radial Basis Function Kernel:\n",
    "    \n",
    "It is also known as RBF kernel. For distance metric, squared euclidean distance is used here.\n",
    "\n",
    "It is used to draw completely non-linear hyperplanes.\n",
    "\n",
    "Unlike Logistic Regression Classifiers, SVM Classifiers donot output probabilities for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optimized Gradient Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to XGBoost, we have optimized the parameters thorugh GridSearch, bringing up the accuracy from 60 to 85%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
